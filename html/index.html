<!DOCTIPE  html>
<html>
    <head>
	    <title> ma page web en html</title>
		<meta charset="utf-8"/>
		<link rel="stylesheet" href="background.css" />
	</head>
	<body>
	<! mon commentaire...>
	<img src="../images/12.jpg"/> <h1>L’Intelligence économique, la veille et l’intelligence artificielle</h1>
	<img src="../images/2.jpg "/> <p>Dans les innovations considérées comme promises à révolutionner notre Monde,<br/> l’intelligence artificielle est classée souvent en tête de liste. Qu’il s’agisse de Deloitte,<br/> de Bloomberg, ou bien de nombreux autres cabinets d’analyse ou de médias, l’intelligence<br/> artificielle est considérée comme la technologie disruptive du moment.<br/>
	Ainsi les laboratoires pharmaceutiques, GAFA et autres startups se sont engouffrés sur la brèche à travers des projets<br/> de décryptage du génome humain, de création de nouvelles molécules pour l’industrie pharmaceutiques,<br/> ou bien encore pour recruter des patients pour des essais cliniques, les exemples ne manquent pas…<br/></p>
	<p id="deuxièmep">Le monde pharmaceutique est loin d’être le seul et tous les secteurs semblent s’emparer dernièrement des technologies<br/> de l’intelligence artificielle pour mener des essais d’intégration dans leurs processus métiers.<br/>
	Au moment où les intelligence artificielles ont battu les meilleurs joueurs de Go au Monde,<br/> où elles ont appris à jouer seules aux échecs, sans autre aide que les règles, et là encore, en quelques heures <br/>d’apprentissage préalable, battus les meilleurs, à l’heure où elles ont appris à se “reproduire“, où elles ont créé <br/>de nouvelles langues en discutant entre elles, et où elles ont, en seulement deux ans, atteint le QI de 47… <br/>A ce moment il est important de se demander quel est l’avenir de ces technologies dans les métiers de la veille et de l’intelligence économique.<br/></p>
	<h2><em>L’intelligence artificielle et la veille</em></h2>
	Avec le recul que je pense avoir sur le métier de meilleure de façon extrêmement pragmatique,<br/> j’identifie un ensemble de tâches plutôt chronophages et pas toutes extrêmement intéressantes pour lesquelles<br/> on est en droit (l’on se doit même) de se poser la question de l’utilité de l’intelligence artificielle pour remplacer l’être humain.<br/>
	<ul>
	<li>La traduction automatique</li>
	</ul>
	<p>Dans les tâches utiles auxquelles le veilleur peut être amené à renoncer se trouve la traduction.<br/> Lorsque l’on surveille le web, les connaissances linguistiques se révèlent un réel atout. <br/>Si l’anglais semble être un impératif pour tout veilleur de part l’importance de la langue anglaise dans les publications scientifiques, <br/>business et plus généralement dans la masse d’informations publiées, les autres langues sont souvent difficiles à intégrer <br/>dans un processus de veille pour des raisons de compétences linguistiques et / ou de coûts.<br/></p>
	<p>Certes les outils de traduction automatique tels que Systran ou  Google Translate, peuvent rendre service,<br/> ils ne parviendront pas a atteindre un certain degré de finesse de compréhension et ne permettront pas la traduction de documents<br/> pour des publications officielles “propres” sans passage par une traduction humaine.<br/> Google l’a d’ailleurs bien compris et propose à sa communauté d’utilisateurs de réaliser des traductions<br/> manuelles pour enrichir sa base de données de traduction à travers son site “community“. <br/>La complexité et la diversité des langues, de leur structures, nécessitent une programmation et / ou un entrainement<br/> spécifique pour les intelligences artificielles orientées traduction ce qui représentait jusqu’à présent un coût important.<br/></p>
	<p>Toutefois, l’investissement des acteurs majeurs du Web en ce domaine a permis une avancée colossale <br/>en termes de qualité de traduction à l’échelle planétaire. L’on notera ainsi que  depuis août 2017  <br/>les traductions proposées par Facebook le sont entièrement par des intelligences artificielles, offrant par rapport<br/> au précédent système de traduction une amélioration de la qualité de la traduction de 11% en s’appuyant sur des réseaux neuronaux capables<br/> d’avoir une mémoire à court terme, de traiter des plus longues séances de mots dans leu ensemble,<br/> de se “débrouiller” avec les mots qu’elle ne connaît pas,…<br/>
Dernière innovation en date, la mise sur le marché d’une oreillette de traduction simultanée.<br/></p>
    <p>envoyez moi un<a href="mailto:darius-niang01@gmail.com">mail</a></p>
    <ul>
	<li>Le nettoyage et la structuration de l’information</li>
	</ul>
	<p>Lorsqu’il parcourt une page web, un article de presse en ligne, l’oeil humain sait faire la différence<br/> entre ce qui correspond au texte de l’article et à tout le contenu périphérique. <br/>Il sait également extrêmement rapidement attribuer une date à l’article ainsi<br/> que son auteur lorsque ces dernières informations sont présentes.</p>
	<p>Pour tout veilleur, l’incapacité des outils actuels à réaliser ces tâches élémentaires pour l’être humain <br/>se traduit par des tâches chronophage et peu intéressantes de vérification et de qualification des métas données, <br/>mais également des tâches de “nettoyage” de l’information. Qu’il s’agisse des bandeaux, des menus, ou de plus en plus<br/> des éléments polluants intégrés en milieu d’article, le veilleur perdra de longues minutes à mettre l’information au propre.<br/></p>
	<p>L’incapacité des outils (NDLR : de la plupart des outils, je ne citerai aucun nom) de veille à réaliser<br/> ces tâches pollue par ailleurs souvent le flux d’information. Lorsque la tache de “zonage” de l’information intéressante<br/> intervient trop tard dans le processus d’extraction et de filtre, les mots clés présents dans les zones périphériques<br/> (header, menu, footer, pubs, “A lire”, A la une, etc) vont polluer le flux.<br/></p>
	<p>Les outils d’intelligence artificielle commencent à se pencher sur ces problématiques, <br/>Diffbot par exemple propose des API permettant ce genre de services (et l’on trouvera également l’API Mercury de Spotlight qui est recommandée par le défunt Readability,<br/> qui était un exemple en termes d’extraction d’article ciblé) mais, si ses performances sont intéressantes sur des articles de presse,<br/> elles restent moyennes sur des articles scientifiques en ligne par exemple. Par ailleurs ces outils d’intelligence artificielle<br/> reposent sans toujours le dire sur l’analyse du code, des templates et des “objets” à l’intérieur du code source des pages web.<br/> Sans connaissance ou apprentissage préalable du modèle ou des “objets”, l’AI échouera souvent.<br/>
	A ce jour toutefois l’on note relativement peu (voire pas) de projets impliquant l’intelligence artificielle<br/> sur ce type de fonction et encore moins de solutions qui l’implémentent dans un processus complet de crawling,<br/> de structuration, de filtre et de publication.<br/></p>
	<ul>
	<li>La découverte de <strong>sources pertinentes</strong></li>
	</ul>
	<img src="../images/5.png "/>
	<p>Chaque veiller connaît bien la tâche fondamentale de sourcing qui constitue à constituer un bouquets<br/> de sources produisant potentiellement des informations interessantes sur la thématique de surveillance définie. <br/>Dans l’idéal les veilleurs surveilleraient le web dans son entièreté mais à ce jour c’est Google <br/>qui se rapproche le plus d’une infrastructure capable d’indexer le web global et pour cela dispose d’une infrastructure colossale.<br/> Le veilleur et les éditeurs de logiciel de veille doivent bien se faire une raison, dans une processus de veille,<br/> le sourcing correspond à une étape fondamentale permettant de surveiller les sources prioritaires en corrélation direct<br/> avec des contraintes techniques et des contraintes de temps (de traitement de l’information).<br/></p>
	<p>Dans les éditeurs de logiciel de veille, c’est à ma connaissance IXXO (Web Mining) qui a le plus développé technologiquement<br/> une solution de crawl exploratoire, le logiciel partant de points de crawl prioritaires et crawlant les liens en profondeur<br/> (liens internes et externes) et permettant ainsi de découvrir des informations et donc également des sources en relation avec une thématique.<br/></p>
	<p>En dehors de cette solution logicielle, peu des acteurs du secteur proposent un réel module <br/>d’aide à la découverte de sources si ce n’est de proposer leurs propres packages <br/>de sources (reposant souvent sur un sourcing interne de l’éditeur mis en commun avec le sourcing réalisé par leurs clients) <br/>et un éventuel module “Google”, le logiciel se reposant sur des requêtes envoyées au célèbre moteur de recherche afin d’identifier ses dernières actualités.<br/>
	<p>La marge de progrès est également colossale de ce point de vue là… En se reposant sur des sites<br/> tels que ReddIt, Twitter, Google News, ou tout autre outils de republication et de partage d’information,<br/> en couplant un crawler conditionnel et sémantique, en développant des outils de reconnaissance de zones textuelles intéressantes<br/> et de détection de dates performants, et enfin en ajoutant un module de machine learning voire d’intelligence artificielle<br/> pour apprendre avec l’utilisateur ce qu’est une source intéressante à intégrer au projet de veille, <br/>la fonctionnalité pourrait tout simplement être révolutionnaire pour nos métiers. Un périmètre de veille auto-apprenant qui détecterait et<br/> corrigerait les liens morts, apprendrait à ajuster ses paramétrages d’extraction et enfin proposerait l’ajout de sources semi-automatiques. <br/>Il n’est pas interdit de rêver.<br/></p>
	<h3>L’avenir de nos métiers face à <mark>l’intelligence artificielle</mark></h3>
	<p>A ce stade de l’état d’avancement des technologies d’intelligence artificielle,<br/> il me semble que le métier d’analyste est loin d’être remis en question. La capacité des intelligence artificielles à réaliser <br/>des synthèses de sujets complexes et parfois abstrait reste aujourd’hui limité mais reste limité aussi car l’être humain peine encore à modéliser<br/> et à formaliser le processus d’analyse intellectuelle qui repose sur des connaissances parfois diffuses, des perceptions<br/> inconscientes, et des processus de synthèse et de traitement de l’information complexes à mettre en équation.<br/></p>
	<p>Toutefois il me semble que le métier du veilleur et le positionnement <br/>“paramétrage / sélection / validation” peut être amené à se trouver dans une position <br/>délicate (encore plus délicate que celle d’aujourd’hui). Si aujourd’hui ce dernier est globalement <br/>sauvé par la précarité du business model des logiciels de veille dont le fonds de commerce est globalement incompatible<br/> avec un strict respect des droits d’auteur, les acteurs majeurs de l’information digitale ont redoublé leurs investissements ces dernières années <br/>pour développer des nouvelles technologies sémantiques et d’intelligence artificielle.</p>
	<p>Enfin, je ne peux que militer pour que le métier du veilleur devienne soit plus technicien soit plus analyste.</p>
	<p>Au cours de mes dernières années de travail j’ai constaté la forte numérisation et digitalisation des outils.<br/> Le processus était amorcé de longue date bien évidemment mais l’on voit à quel point ili s’est accéléré.<br/> Aujourd’hui il relève clairement des compétences clés de maîtriser certains fondamentaux liés à l’information digitale <br/>et à son accès : web mining, API, text mining, etc</p>
	<p>Enfin, j’ai vu au fur et à mesure des ans s’appauvrir les effectifs de veilleur qui sont souvent devenus analystes,<br/> disposant d’une double compétence veille + sectorielle. Les veilleurs ont de plus en plus évolué vers des fonctions les amenant à<br/> (pré)analyser l’information et à en fournir une version qualifié, digérée, simplifiée, enrichie.<br/>

Et vous ? Quel est votre avis sur l’intelligence artificielle et son apport à nos métiers ? Quelles lectures intéressantes ou articles vous ont marqués ?
    <ul>
	<li>Intelligence artificielle  : un supercalculateur de 14 pétaflops, à quoi ça sert ?</li>
<p>Le gouvernement a signé ce mois-ci avec Hewlett Packard Entreprise un contrat<br/> pour un puissant supercalculateur destiné à la recherche en intelligence artificielle.</p>
<img src="../images/3.jpg "/>
<ul
<li>Un robot entraîné à la fois dans le virtuel et le réel</li>
</ul>
<img src="../images/4.jpg "/>
<h4><strong>Intelligence artificielle et traitement de l’information : tendances et perspectives</strong><h4>
<p>L’intelligence artificielle et le big data occupent une place de choix dans l’actualité.<br/> La « célèbre revue scientifique » Paris-Match leur a d’ailleurs consacré 3 articles depuis le début de l’année,<br/> c’est dire l’importance mais surtout la démocratisation de ces sujets. Cette discipline touche aujourd’hui la plupart des secteurs d’activité.<br/> Éditeur de logiciel et de solutions de veille stratégique, nous travaillons au quotidien sur ces problématiques d’Intelligence Artificielle<br/> et de Big Data sans jamais perdre de vue les processus de travail de nos utilisateurs et les attentes de nos clients.<br/> Ces problématiques sont aujourd’hui au cœur même de nos réflexions et de nos innovations.<br/>

L’intelligence Artificielle et le big data modèlent et dirigent notre réflexion quant aux orientations de notre produit<br/> et de notre approche des projets de veille stratégique, elles bouleversent en profondeur tous les aspects de notre métier et de celui de nos clients.<br/></p>
<ul>
<li>L’Intelligence Artificielle, de quoi parle-t-on ?</li>
<img src="../images/6.jpg "/> <p> Dans l’actualité récente, la meilleure illustration des progrès de l’intelligence artificielle<br/> est sans doute le programme AlphaGo et ses successeurs. Alpha Go a été développé par Deepmind filiale de Google en 2015 <br/>afin de battre les meilleurs joueurs de Go. Le 27 mai 2017 il battra le champion du monde Ke Jie, mettant fin à sa carrière…<br/>

Mais une disruption d’ampleur a permis la conception du logiciel AlphaGo Zero, <br/>logiciel qui a battu son prédécesseur AlphaGo 100 parties à 0 et ce en ayant bénéficié uniquement d’un auto-apprentissage !<br/></p>
<p>Les algorithmes « intelligents » d’AlphaGo étaient basés sur des techniques d’apprentissage profond combinées<br/> à la théorie des graphes et fondées sur l’analyse de quantités importantes de données (ensemble des parties jouées par les grands maîtres).<br/>

Les algorithmes avaient ensuite pour tâche d’étudier ces données, pour évaluer une situation tactique<br/> d’un point de vue stratégique, et déterminer la valeur d’un coup au regard de la stratégie globale, <br/>qu’on peut résumer par « gagner ». Les algorithmes étaient ainsi capables d’examiner les conséquences stratégiques de chaque coup<br/> envisageable, sur une profondeur inaccessible à l’esprit humain afin d’arbitrer et choisir le coup tactique<br/> le plus à même de les rapprocher de l’objectif. Ce type d’algorithme s’appuie sur des concepts de big data <br/>en raison de la masse d’informations existantes manipulées pour gérer une situation et entreprendre des actions.<br/></p>
<p>AlphaGo Zero a changé la donne en mettant en œuvre un système dont la base informationnelle est « vide »,<br/> constituée uniquement des règles formalisées du jeu de go. Le logiciel a ensuite exploré le jeu de manière autonome, jusqu’à<br/> arriver à déterminer lui-même à la fois la stratégie d’ensemble et les coups tactiques permettant de mettre en œuvre cette stratégie.<br/> AlphaZero va encore un peu plus loin car il s’agit d’une version générique (Jeu de Go, Shogi, échecs).<br/> On parle alors d’algorithme d’apprentissage par renforcement non supervisé. C’est pourquoi nous voyons là <br/>un exemple d’Intelligence Artificielle « Forte », se démarquant des approches « classiques » de l’intelligence artificielle, fondées<br/> sur des modèles d’apprentissage à base de masses d’informations qualifiées destinées à l’entrainement.<br/></p>
<h5>L’intelligence Artificielle : quels apports pour la veille stratégique ?<h5>
<img src="../images/7.jpg "/> <p>Des applications de ces concepts dans le domaine de la veille stratégique reviendraient à analyser<br/> des données en nombre, puis d’y rechercher des motifs récurrents, des éléments sémantiquement ou statistiquement <br/>signifiants au regard des modèles existants, pour finalement arriver à une conclusion d’identification d’une <br/>relation, d’un comportement ou de l’apparition d’un acteur par exemple. <br/>Mais tout cela est basé sur des concepts et technologies existant depuis des années, on parle ici <br/>de modèle d’apprentissage, d’analyse de « pattern », d’utilisation d’ontologies ou d’analyse de sentiment (liste non exhaustive !).<br/> Tout cela repose sur une masse plus ou moins importante de données, que cela soit pour le référentiel initial <br/>ou pour les informations à traiter pour repérer des récurrences ou autres.<br/></p>
<p>Dans un projet de veille stratégique, il est nécessaire d’analyser et de qualifier un ensemble d’information souvent hétérogène.<br/> Il est difficile de prévoir quelle technologie sera mise au point et commercialisée par un concurrent ou <br/>quelle voie stratégique sera choisie par un acteur du marché sans disposer d’une base informationnelle.<br/> Pour faire une analogie avec AlphaGo Zero, en formalisant les règles générales du jeu du marché et de l’économie sans autre<br/> entrant, une intelligence artificielle serait peut-être capable de déterminer de grandes tendances macro-économiques ou sociétales.<br/> Mais sur la base du célèbre et rebattu modèle de Porter, cela ne me serait pas d’une grande utilité pour ma survie au sein de mon environnement,<br/> entre mes clients, mes fournisseurs, mes produits, mes concurrents et de nouveaux entrants.<BR/></P>
<P>Pour les lecteurs d’Isaac Asimov, la puissance de la psychohistoire du cycle de Fondation tient autant<br/> à la compréhension des règles du jeu qu’à la prise en compte de tout l’existant informationnel à un instant t.<br/> Et si elle est capable de prédire les grandes tendances et les réactions globales à certains événements, elle s’avoue incapable<br/> de prédire les comportements d’individus isolés. Or c’est justement ce que nous intéresse ici. <br/>Nous avons besoin de connaitre la tendance (que va devenir ce marché ?) mais aussi les comportements individuels (que va faire mon concurrent ?)<br/></p>
<p>C’est pourquoi, dans le contexte de la veille stratégique, nous accepterons volontiers d’étendre la notion <br/>d’Intelligence Artificielle en sortant du nouveau cadre défini par AlphaGo Zero, en y englobant « tout mécanisme, procédé ou fonction<br/> permettant de dépasser le cadre des connaissances actuelles pour en créer ou en suggérer de nouvelles ».<br/>

Cette définition est d’ailleurs cohérente avec la déclaration de Yann Le Cun, patron de l’Intelligence Artificielle chez Facebook,<br/> dans une interview donnée à la célèbre revue Paris Match : « L’Intelligence artificielle vise à démultiplier l’intelligence humaine »<br/>

Cela va donc nous permettre de parler d’intelligence artificielle en même temps que d’intelligence économique, de veille <br/>stratégique ou d’intelligence technologique. La composante des big data est dans ce contexte au cœur de l’équation.<br/>

Cependant un changement d’échelle est nécessaire pour profiter de l’Intelligence Artificielle dans notre domaine.<br/></p>
<h6>Concrètement en quoi l’Intelligence Artificielle pourrait-elle nous aider ?</h6>

<img src="../images/8.jpg "/><p>La première application de ces technologies devrait nous permettre en premier lieu d’identifier les « concepts »<br/> pertinents au sein d’une masse d’information. Par concept nous entendons d’une manière plus prosaïque les entités nommées, qu’elles<br/> représentent des personnes, des pays, des sociétés, des technologies, des matériaux, etc…<br/>

Les méthodes et technologies de détection des entités nommées existent depuis de nombreuses années, mais restent confrontées à de<br/> nombreux problèmes techniques qui les empêchent de pouvoir être véritablement qualifiées d’intelligence artificielle.<br/></p>
<ul>
<li><strong>La qualité et fiabilité des données</strong></li>
<p>Le premier problème posé, d’ailleurs identifié dans l’article d’Actulligence, est celui de la qualité<br/> des données dans un contexte de données non-structurées. La collecte d’un grand nombre d’informations sur un sujet précis<br/> est aisée, mais quid du contenu global et de son exploitabilité en mode automatique. La page d’un article en ligne contient <br/>certes le contenu intéressant mais aussi une grande quantité d’information au mieux inutile, au pire erronée. <br/>Les éléments de navigation du site vont contenir des mots-clés, potentiellement des noms de société, des pays, de même<br/>que les nuages de tags, de même que les news liées ou récemment publiées, qui n’ont pas de rapport direct avec le sujet qui nous occupe.<br/> Cela peut être amélioré par des algorithmes utilisant la structure du site pour dépolluer l’information, mais ce n’est<br/> pas une solution absolue ; d’ailleurs le contenu signifiant peut lui aussi comporter des signaux contradictoires.<br/>Certaines entités nommées peuvent difficilement être identifiées par les moyens à notre disposition. <br/>Par exemple : dans une étude d’influence, déterminer le pays d’origine des informations/publications au sein d’une grande masse documentaire. <br/>Il est extrêmement simple d’identifier les pays cités, mais comment identifier le pays dont émane l’information ? <br/>La langue peut donner une indication, l’extension du domaine également (bien que de moins en moins)<br/> ou l’adresse IP, bien que falsifiable, mais il faut aller au-delà.</p>
Lorsque l’auteur de l’information est identifiable (problématique de l’entité nommée « Auteur », simple <br/>dans le contexte de publications structurées, complexe dans tous les autres cas…), il devient possible d’interroger <br/>un hypothétique « référentiel de la connaissance existante » dans lequel on dispose de la nationalité de la personne.<br/> Encore faut-il disposer de ce référentiel, et que celui-ci soit en perpétuelle évolution. <br/>Et bien évidemment, se mettre en conformité avec la Loi Informatique et Libertés et autres encadrements des données collectables et <br/>stockables, ce qui peut s’avérer plus bloquant (à juste titre) que les problématiques technologiques.<br/></p>
<ul>
<li>La détection de concepts</l>
</ul>
<p>Il en va de même de la détection de concepts plus liés au domaine d’activité, comme des technologies, des composants, des matériaux, etc…</p>
<p>La réponse la plus communément apportée à cette problématique est celle de l’analyse sémantique par le biais <br/>entre autres de thésaurus et d’ontologies. Il y a deux principaux écueils à ce type de réponse.<br/></p>
<p>Le premier est qu’il faut disposer d’outils sémantiques ou de modèles d’apprentissage spécialisés dans le domaine traité.<br/> Dans la chimie, l’ontologie et les thésaurus contiendront les molécules et leur formulation, les types de réaction, etc…<br/> Dans la micro-électronique certains référentiels auront probablement une part de recoupement avec les précédents, mais concernant les technologies <br/>ou les composants, la divergence sera énorme. D’où la nécessité de développer et de maintenir ces outils spécialisés avec le (fort) coût technologique et humain associé.<br/></p>
<p>Nous avons déjà eu l’occasion de dire que l’apport tangible de l’intelligence artificielle dans ce domaine serait<br/> de fournir au moins une assistance pertinente dans la constitution de ces modules sémantiques.<br/> Le problème n’est pas simple, plusieurs projets dont certains au niveau européen se sont penchés sur cette problématique, et<br/> les solutions concrètes et opérationnelles n’ont pas encore émergé.<br/></p>
<p>On se trouve ici face à un challenge de l’ordre de celui résolu par AlphaGo Zero, à savoir partir d’une feuille blanche sur<br/> un sujet, hormis par exemple les règles de construction de la syntaxe et de la grammaire (les règles du jeu donc), et d’obtenir <br/>un système capable d’exploiter un corpus documentaire pour en extraire le thésaurus des termes significatifs du domaine et sa représentation <br/>hiérarchique organisée, autrement dit une ontologie.<br/></p>
<p>Le système peut ici difficilement être autonome et apprendre à évaluer seul la valeur de ce qu’il fait.<br/> Au mieux, il délivrera une version « brouillon » de la connaissance, qui devra être reprise,<br/> corrigée et enrichie par un expert pour arriver à une version fiable.<br/></p>
 <p>Cet ensemble d’opérations permettra de constituer à terme un modèle d’apprentissage spécialisé pour <br/>le domaine, mais l’intervention d’un analyste reste encore essentielle et centrale, et <br/>contribue d’ailleurs à constituer et enrichir le référentiel de « la connaissance existante ».<br/></p>
<p>D’autres problématiques quotidiennes auxquelles sont confrontés les professionnels <br/>de la veille stratégique peuvent bénéficier des apports de l’intelligence artificielle. Nous poursuivrons cette revue prochainement dans un prochain article.
	</body>
		
</html>